{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of characters is 185\n",
      "The maximum number of words is 31\n"
     ]
    }
   ],
   "source": [
    "## 数据准备\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual/data/english\")\n",
    "data[\"val\"] = data[\"validation\"]\n",
    "del data[\"validation\"]\n",
    "import pandas as pd\n",
    "data[\"train\"].to_pandas().label.value_counts()\n",
    "# 字符数\n",
    "max_char = data['train'].to_pandas()['text'].str.len().max()\n",
    "print(f\"The maximum number of characters is {max_char}\")\n",
    "# 词数\n",
    "max_words = data['val'].to_pandas()['text'].str.split().str.len().max()\n",
    "print(f\"The maximum number of words is {max_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words is 31\n",
      "The maximum number of words is 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'okay i\\\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\\\u2019mon America aren\\\\u2019t you sick of her yet? (sorry) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = data['val'].to_pandas()['text'].str.split().str.len().max()\n",
    "print(f\"The maximum number of words is {max_words}\")\n",
    "max_words = data['test'].to_pandas()['text'].str.split().str.len().max()\n",
    "print(f\"The maximum number of words is {max_words}\")\n",
    "data[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "llama_path = \"./Meta-Llama-3.1-8B\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_path, add_prefix_space=True)\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "def preprocessing_function(examples):\n",
    "    examples['label'] = [int(i) for i in examples['label']]\n",
    "    return llama_tokenizer(examples['text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "tokenized_data = data.map(preprocessing_function, batched=True, remove_columns= [\"text\"])\n",
    "tokenized_data.set_format(\"torch\")\n",
    "# 创建 DataCollatorWithPadding 实例\n",
    "data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141f8a03482c4d0a8187adabbcc161c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "pretrain_model = AutoModelForCausalLM.from_pretrained(llama_path, \n",
    "                                                                 num_labels=3,\n",
    "                                                                device_map=\"auto\",\n",
    "                                                                offload_folder=\"offload\",\n",
    "                                                                trust_remote_code=True)\n",
    "pretrain_model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "# llama_model.config.use_cache = False\n",
    "# llama_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9b80e8997147b0beeebe8f70727222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from transformers.utils.import_utils import is_torch_bf16_gpu_available\n",
    "\n",
    "\n",
    "model_path = \"./Meta-Llama-3.1-8B\"\n",
    "\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the founder of the company which published Big Picture (Magazine)? - General Knowledge (\n"
     ]
    }
   ],
   "source": [
    "main_prompt = \"\"\"\n",
    "Wikipedia Title: Big Picture (magazine)\n",
    "The Big Picture series provides teachers and post-16 students with up-to-date information on research findings in biology and medicine, and the social and ethical implications of this research. Published by the Wellcome Trust as a free educational resource, each issue is available for free electronically. The website provides free resources for teachers and online activities for students, including lesson ideas, animations, image galleries and short videos. , the print subscription has been discontinued and replaced with online only.\n",
    "\n",
    "Wikipedia Title: Picture Play (magazine)\n",
    "Picture Play, originally titled Picture-Play Weekly was an American weekly magazine focusing on the film industry. Its first edition was published on April 10, 1915. It eventually transitioned from a weekly to a monthly magazine, before ending its production run in 1941 when it merged with \"Charm Magazine\".\n",
    "\n",
    "Wikipedia Title: Warren Publishing\n",
    "Warren Publishing was an American magazine company founded by James Warren, who published his first magazines in 1957 and continued in the business for decades. Magazines published by Warren include \"After HoursCreepyEerieFamous Monsters of FilmlandHelp!\", and \"Vampirella\". Initially based in Philadelphia, Pennsylvania, the company moved by 1965 to New York City.\n",
    "\n",
    "Wikipedia Title: The Big Picture\n",
    "The Big Picture( a top down perspective on a problem or situation) may refer to:\n",
    "\n",
    "Wikipedia Title: Current Publishing (UK)\n",
    "Current Publishing is a British magazine publishing company based in Chiswick, London.\n",
    "\n",
    "Wikipedia Title: Highline Big Picture\n",
    "Highline Big Picture is a small school in the Highline School District in SeaTac, Washington, that is part of a network of Big Picture schools all over the United States. The high school opened 2005 with 2009 being the first year with a graduating class. At Highline Big Picture the 9th graders are called 101's, 10th graders 201's, 11th graders 301's and 12th graders 401's. Highline Big Picture, as part of the Big Picture Network, practices Learning Through Internships or LTI. On Tuesdays and Thursdays students go to internships in their field of interest. The school currently consists of 120 students. On Mondays, Wednesdays, and Fridays students attend one class a day which contains, at most, 17 students. These classes are called an advisory.\n",
    "\n",
    "Wikipedia Title: James Warren (politician)\n",
    "James Warren (September 28, 1726 – November 28, 1808) was the President of the Massachusetts Provincial Congress and a Paymaster General of the Continental Army during the American Revolutionary War, among other positions. He is also famous as the husband of Mercy Otis Warren and for his outspoken courage as an Anti-Federalist. General James Warren is sometimes confused with the two other Massachusetts Revolutionaries, the brothers Joseph Warren and John Warren, but they were not at all closely related, merely had similar names and views.\n",
    "\n",
    "Wikipedia Title: Alan Warren Friedman\n",
    "Alan Warren Friedman is Thaman Professor of English and Comparative Literature in the department of English at the University of Texas at Austin. He is a specialist in the work of James Joyce and Samuel Beckett.\n",
    "\n",
    "Wikipedia Title: Warren High School (Warren, Texas)\n",
    "Warren High School is a high school in Warren, Texas. It is part of the Warren Independent School District.\n",
    "\n",
    "Wikipedia Title: Warren Benbow\n",
    "Warren Benbow( born December 22, 1954, in New York City) is a drummer who has worked with Nina Simone, Jimmy Owens, Larry Willis, Eddie Gómez, Olu Dara, Michael Urbaniak, Teruo Nakamura, and was an original member of James Blood Ulmer's band' Odyssey'.\n",
    "\n",
    "Wikipedia Title: The Warrens of Virginia\n",
    "The Warrens of Virginia can refer to:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question_prompt = \"\"\"\n",
    "Q: Answer the following question by reasoning step-by-step..\n",
    "Who is the founder of the company which published Big Picture (Magazine)?\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = question_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "all_generate_ids = model.generate(\n",
    "    input_ids.input_ids,\n",
    "    max_length=100,                 # 生成的最大token数\n",
    "    min_length=1,                   # 生成的最小token数\n",
    "    do_sample=False,                # 是否进行采样\n",
    "    eos_token_id=tokenizer.encode(\"\\n\")[0],  # 结束符\n",
    "    temperature=1.0,                # 温度参数，控制生成的随机性\n",
    "    top_k=50,                       # top-k 采样\n",
    "    top_p=1.0,                      # top-p (nucleus) 采样\n",
    "    num_return_sequences=1,         # 返回的序列数量\n",
    "    repetition_penalty=None,        # 重复惩罚\n",
    "    length_penalty=None,            # 长度惩罚\n",
    ")\n",
    "\n",
    "generated_ids = all_generate_ids[:, input_ids.input_ids.shape[-1]:]\n",
    "# generated_ids = all_generate_ids\n",
    "\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
